{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e3fb5-fb7f-45ec-8372-09a4eb547a25",
   "metadata": {
    "id": "4f3e3fb5-fb7f-45ec-8372-09a4eb547a25",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import itertools\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b6b39-d20d-4665-b187-53a438ec4c76",
   "metadata": {},
   "source": [
    "Disclaimer: due to hardware limitations, I had to train the model with an under sample from the dataset. The complete set would have been used otherwise, as well as further optimization of hyper parameters and aditional machine learning models (randomforest, svc), and also a recurrent neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8831b3d-e0fd-4065-8516-179c292d0240",
   "metadata": {},
   "source": [
    "Index:\n",
    "    1 - Data preprocesing\n",
    "    \n",
    "    2 - Looking for patterns and relations between fraudulent transactions\n",
    "    \n",
    "    3 - Training the models\n",
    "    \n",
    "    4 - Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ed9fe-9b07-4708-87a5-af8fa6c0838f",
   "metadata": {
    "id": "ad0ed9fe-9b07-4708-87a5-af8fa6c0838f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Fraud.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039e847-ee6a-4bb7-bd54-6ebe1613de2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f039e847-ee6a-4bb7-bd54-6ebe1613de2e",
    "outputId": "c6007b26-fabd-4177-b57e-f37eddb46a16",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a5995-e44a-4dfd-a1da-dfbea365fa63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "de2a5995-e44a-4dfd-a1da-dfbea365fa63",
    "outputId": "5772224d-5e71-4750-a536-7e67274a7fb5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76358b64-e06e-426d-b4df-b74e24dd92ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76358b64-e06e-426d-b4df-b74e24dd92ee",
    "outputId": "b8785722-589e-4c51-df7d-3d7216a3e801",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter rows that nameDest starts with 'M' (Merchant)\n",
    "merchant_rows = df[df['nameDest'].str.startswith('M')]\n",
    "\n",
    "print(merchant_rows['newbalanceDest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce15bb96-d96b-421a-90ee-f218537df1cc",
   "metadata": {},
   "source": [
    "At first glance, the dataset had no missing values, so no null treatment was done. However, given that Merchant movements doesn't have meaningful information, all the M customers were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973390ce-61e1-4690-a561-f3ba73f01904",
   "metadata": {
    "id": "973390ce-61e1-4690-a561-f3ba73f01904",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[~df['nameDest'].str.startswith('M')]\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698a1f4-c92f-438e-97dc-3878cad9fbfe",
   "metadata": {
    "id": "8698a1f4-c92f-438e-97dc-3878cad9fbfe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by='step')\n",
    "\n",
    "train_split = 575\n",
    "\n",
    "train = df[df['step'] <= train_split]\n",
    "test = df[df['step'] > train_split]\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1cbf8-00e5-481f-a8de-586389802793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['train_test'] = 1\n",
    "test['train_test'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4e744-0c6b-4ef4-8214-b54b04f3902f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6c4e744-0c6b-4ef4-8214-b54b04f3902f",
    "outputId": "595dd020-a192-4419-dcf9-4725512fd1d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_count = df['isFraud'].sum()\n",
    "false_count = len(df) - true_count\n",
    "\n",
    "print(f'True values (1): {true_count}')\n",
    "print(f'False values (0): {false_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f49bdd-e320-43cf-b6c7-0497a879cdca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_transformations(df):\n",
    "    categories = ['CASH_IN', 'CASH_OUT', 'TRANSFER', 'DEBIT']  # Lista de categorías conocidas\n",
    "    \n",
    "    # Reemplazamos cualquier categoría desconocida por 'PAYMENT' (o la categoría que desees)\n",
    "    df['type'] = df['type'].apply(lambda x: x if x in categories else 'PAYMENT')\n",
    "    \n",
    "    category_mapping = {'CASH_IN': 1, 'CASH_OUT': 2, 'TRANSFER': 3, 'DEBIT': 4}\n",
    "    \n",
    "    df['type_encoded'] = df['type'].map(category_mapping)\n",
    "    \n",
    "    numeric_columns = ['step', 'amount', \n",
    "                       'oldbalanceOrg', 'newbalanceOrig', \n",
    "                       'oldbalanceDest', 'newbalanceDest']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    df_encoded = df.drop(['type'], axis=1)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Aplicar transformaciones a los conjuntos de datos\n",
    "training_final = apply_transformations(train.copy())\n",
    "test_final = apply_transformations(test.copy())\n",
    "\n",
    "\n",
    "\n",
    "numeric_columns = ['step', 'amount',\n",
    "                   'oldbalanceOrg', 'newbalanceOrig',\n",
    "                   'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = training_final.drop(['isFraud', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1)\n",
    "X_train_scaled[numeric_columns] = scaler.fit_transform(X_train_scaled[numeric_columns])\n",
    "y_train = training_final['isFraud']\n",
    "X_train_scaled.columns = X_train_scaled.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf1d96-7632-451a-acf4-1866bc9b9194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Imputación y escalado\n",
    "preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train_scaled_imputed = preprocessor.fit_transform(X_train_scaled)\n",
    "\n",
    "# Sobremuestreo con SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.0075, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled_imputed, y_train)\n",
    "\n",
    "# Convertir a DataFrames\n",
    "X_train_resampled_df = pd.DataFrame(X_train_resampled, columns=X_train_scaled.columns)\n",
    "y_train_resampled_df = pd.Series(y_train_resampled, name='isFraud')\n",
    "\n",
    "# Crear conjunto de prueba\n",
    "X_test_scaled = test_final.drop(['isFraud', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1)\n",
    "X_test_scaled.columns = X_test_scaled.columns.astype(str)\n",
    "y_test = test_final['isFraud']\n",
    "\n",
    "# Mostrar información sobre el tamaño del conjunto de entrenamiento y prueba\n",
    "print(\"Train set transactions:\", X_train_resampled_df.shape[0])\n",
    "print(\"Test set transactions:\", X_test_scaled.shape[0])\n",
    "\n",
    "# Procesamiento por lotes para el conjunto de entrenamiento\n",
    "batch_size = 10000\n",
    "num_batches = len(X_train_resampled_df) // batch_size\n",
    "\n",
    "X_train_batches = []\n",
    "y_train_batches = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = (i + 1) * batch_size\n",
    "    X_batch = X_train_resampled_df.iloc[start_idx:end_idx, :]\n",
    "    y_batch = y_train_resampled_df.iloc[start_idx:end_idx]\n",
    "    X_train_batches.append(X_batch)\n",
    "    y_train_batches.append(y_batch)\n",
    "\n",
    "# Si hay un lote final con un tamaño menor que batch_size\n",
    "if len(X_train_resampled_df) % batch_size != 0:\n",
    "    X_batch = X_train_resampled_df.iloc[num_batches * batch_size:, :]\n",
    "    y_batch = y_train_resampled_df.iloc[num_batches * batch_size:]\n",
    "    X_train_batches.append(X_batch)\n",
    "    y_train_batches.append(y_batch)\n",
    "\n",
    "# Combinar todos los lotes\n",
    "X_train_final = pd.concat(X_train_batches)\n",
    "y_train_final = pd.concat(y_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    return tf.keras.models.load_model(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo\n",
    "loaded_model = load_model('modelo_entrenado.h5')\n",
    "\n",
    "def create_sequence_data(df, sequence_length):\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(df) - sequence_length):\n",
    "        # Obtén las secuencias de características\n",
    "        X.append(df.iloc[i:i+sequence_length, :].values)\n",
    "        \n",
    "        # Obtén la etiqueta de la última fila en la secuencia\n",
    "        y.append(df.iloc[i+sequence_length-1, -1])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Realiza predicciones en los datos de prueba\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Ajusta las dimensiones de y_test\n",
    "    y_test_reshaped = y_test.values.reshape(-1, 1)\n",
    "\n",
    "    # Evalúa el rendimiento del modelo (puedes usar diferentes métricas según tu problema)\n",
    "    mse = mean_squared_error(y_test_reshaped, y_pred)\n",
    "    print(f'Mean Squared Error on Test Data: {mse}')\n",
    "\n",
    "\n",
    "# Ajustar la longitud de las secuencias de prueba a 10\n",
    "sequence_length = 10\n",
    "X_test_scaled_reshaped, y_test = create_sequence_data(X_test_scaled, sequence_length)\n",
    "\n",
    "# Asegurémonos de que las dimensiones sean correctas\n",
    "X_test_scaled_reshaped = np.reshape(X_test_scaled_reshaped, (X_test_scaled_reshaped.shape[0], sequence_length, X_test_scaled_reshaped.shape[2]))\n",
    "\n",
    "# Evaluar el modelo cargado\n",
    "evaluate_model(loaded_model, X_test_scaled_reshaped, y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
